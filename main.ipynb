{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c26465c",
   "metadata": {},
   "source": [
    "<img src = \"./image/image.png\" width=100% heigth=200px ><br>\n",
    "# <center> Employee Attrition </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d9a8ed",
   "metadata": {},
   "source": [
    "# Introduction<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034ca442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import pandas as pd, numpy as np, seaborn as sns, plotly.express as px, matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a9bbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Customization\n",
    "plt.figure(figsize = (15,9))\n",
    "plt.style.context('bmh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b4a461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading dataset\n",
    "df = pd.read_csv('data.csv')\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show data set information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7315a31",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(f'The dataset has {df.shape[0]} rows for each employee and {df.shape[1]} attributes\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLEANING DATA SET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Dropped columns with more than 90.0% empty values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97562e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df is your DataFrame\n",
    "threshold = 0.8  # Set the threshold for empty values (NaN)\n",
    "\n",
    "# Calculate the threshold for number of non-NaN values\n",
    "num_threshold = int((1 - threshold) * len(df))\n",
    "\n",
    "# Drop columns with more NaN values than the threshold\n",
    "df_cleaned = df.dropna(axis=1, thresh=num_threshold)\n",
    "\n",
    "# Print the columns that were dropped\n",
    "dropped_columns = set(df.columns) - set(df_cleaned.columns)\n",
    "print(f\"Dropped columns with more than {threshold * 100}% empty values: {dropped_columns}\")\n",
    "\n",
    "# Display the shape of the cleaned DataFrame\n",
    "print(\"Cleaned DataFrame shape:\", df_cleaned.shape)\n",
    "\n",
    "# df_cleaned now contains the DataFrame with columns having less than the specified threshold of empty values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Encode categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703ecac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0263ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "# Identify categorical columns\n",
    "categorical_columns = df.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Encode categorical columns\n",
    "df[categorical_columns] = df[categorical_columns].apply(lambda col: encoder.fit_transform(col.astype(str)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Create Gaussian distribution and remove unneseccery attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# Copy the original DataFrame for comparison\n",
    "df_original = df.copy()\n",
    "# Normalize all data using StandardScaler and overwrite the original DataFrame\n",
    "scaler = StandardScaler()\n",
    "df[df.columns] = scaler.fit_transform(df)\n",
    "\n",
    "# Set up the figure and axes\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# List to store attributes to be excluded\n",
    "exclude_attributes = []\n",
    "\n",
    "# Loop through all columns (attributes) and create histograms with Gaussian fits\n",
    "for attribute in df.columns:\n",
    "    # Create a histogram\n",
    "    sns.histplot(df[attribute], kde=True, stat='density',)\n",
    "\n",
    "    # Fit a Gaussian distribution to the normalized data\n",
    "    mu, std = norm.fit(df[attribute])\n",
    "    xmin, xmax = plt.xlim()\n",
    "    x = np.linspace(-4, 4, 100)  # Set x-axis range from -4 to 4\n",
    "    p = norm.pdf(x, mu, std)\n",
    "\n",
    "    # Plot the Gaussian fit for the normalized data\n",
    "    plt.plot(x, p, linewidth=2)\n",
    "\n",
    "    # Check if the standard deviation is below the threshold (0.8)\n",
    "    if std < 0.8:\n",
    "        exclude_attributes.append(attribute)    \n",
    "\n",
    "# Plot the Gaussian distribution for normalized data (mean=0, std=1)\n",
    "x_norm = np.linspace(-10, 10, 100)\n",
    "p_norm = norm.pdf(x_norm, 0, 1)\n",
    "plt.plot(x_norm, p_norm, 'k--', linewidth=2, label='Normalized Gaussian')\n",
    "\n",
    "# Drop excluded attributes from the original DataFrame 'df'\n",
    "df.drop(exclude_attributes, axis=1, inplace=True)\n",
    "\n",
    "# Display excluded attributes\n",
    "print(f\"Excluded attributes with std < 0.8: {exclude_attributes}\")\n",
    "\n",
    "plt.title('Histograms with Gaussian Fits for All Attributes (Normalized)')\n",
    "plt.xlabel('Normalized Values')\n",
    "plt.ylabel('Density')\n",
    "plt.xlim(-4, 4)  # Set x-axis limits from -10 to 10\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Print the shape of the filtered DataFrame\n",
    "print(df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f89878",
   "metadata": {},
   "source": [
    "*JobLevel* doesn't have a description containing its values, so I'll leave them numeric, just as they've been originally collected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfd1899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing target variable classes and its distribution among the dataset\n",
    "fig = px.pie(df, names = 'Attrition', title = 'Target Variable: Attrition', template = 'plotly_dark')\n",
    "fig.update_traces(rotation=90, pull = [0.1], textinfo = \"percent+label\")\n",
    "fig.show('png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA Feature Extraction With Variance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y = df['Attrition']\n",
    "X = df.drop('Attrition', axis=1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_standardized = scaler.fit_transform(X)\n",
    "\n",
    "# Encoding categorical features for PCA\n",
    "# Handle categorical variables appropriately\n",
    "y = encoder.fit_transform(y)\n",
    "\n",
    "# Applying PCA\n",
    "pca = PCA(n_components=2)  # You can adjust the number of components as needed\n",
    "X_pca = pca.fit_transform(X_standardized)\n",
    "\n",
    "# Creating a new DataFrame with PCA components\n",
    "pca_df = pd.DataFrame(data=X_pca, columns=['PC1', 'PC2'])\n",
    "pca_df['Attrition'] = y\n",
    "print(pca_df.shape)\n",
    "\n",
    "# Accessing mean, covariance matrix, eigenvalues, and eigenvectors\n",
    "mean_values = pca.mean_\n",
    "covariance_matrix = pca.get_covariance()\n",
    "explained_variance = pca.explained_variance_\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "eigenvectors = pca.components_\n",
    "\n",
    "# Print or use the results as needed\n",
    "# print(\"Mean Values:\", mean_values)\n",
    "# print(\"Covariance Matrix:\\n\", covariance_matrix)\n",
    "# print(\"Explained Variance:\", explained_variance)\n",
    "# print(\"Explained Variance Ratio:\", explained_variance_ratio)\n",
    "# print(\"Eigenvectors:\\n\", eigenvectors)\n",
    "\n",
    "# Visualizing the PCA components\n",
    "plt.scatter(pca_df['PC1'], pca_df['PC2'], c=pca_df['Attrition'], cmap='viridis')\n",
    "plt.title('PCA Components')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.show()\n",
    "\n",
    "# Splitting data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Standardizing and applying PCA on training set\n",
    "X_train_standardized = scaler.transform(X_train)\n",
    "X_train_pca = pca.transform(X_train_standardized)\n",
    "\n",
    "# Standardizing and applying PCA on testing set\n",
    "X_test_standardized = scaler.transform(X_test)\n",
    "X_test_pca = pca.transform(X_test_standardized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_confu(y_test_pred,model):\n",
    "    # Create a confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "    print('Accuracy: %.2f%%' % (accuracy_score(y_test, y_test_pred) * 100 ))\n",
    "    print('Precision: %.2f%%' % (precision_score(y_test, y_test_pred) * 100))\n",
    "    print('Recall: %.2f%%' % (recall_score(y_test, y_test_pred) * 100))\n",
    "    print('F1_Score: %.2f%%' % (f1_score(y_test, y_test_pred) * 100))\n",
    "    confusion_matrix_model = confusion_matrix(y_test, y_test_pred)\n",
    "    plt.figure(figsize=(12,8))\n",
    "    ax = plt.subplot()\n",
    "    sns.heatmap(confusion_matrix_model, annot=True, fmt='g', ax = ax)\n",
    "    ax.set_xlabel('Predicted Label')\n",
    "    ax.set_ylabel('Actual Label')\n",
    "    ax.set_title(f'Confusion Matrix - {model}')\n",
    "    ax.xaxis.set_ticklabels(['0','1'])\n",
    "    ax.yaxis.set_ticklabels(['0','1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean-Shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MeanShift\n",
    "# Create a MeanShift classifier\n",
    "mean_shift_classifier = MeanShift()\n",
    "\n",
    "# Train the MeanShift classifier on the PCA-transformed features\n",
    "mean_shift_classifier.fit(X_train_pca)\n",
    "\n",
    "# Predictions on the test set\n",
    "y_test_pred = mean_shift_classifier.predict(X_test_pca)\n",
    "\n",
    "# This example demonstrates how to use Mean Shift; however, accuracy might not be the most suitable metric.\n",
    "unique_labels = np.unique(y_test_pred)\n",
    "num_clusters = len(unique_labels)\n",
    "cluster_dict = {unique_labels[i]: i for i in range(num_clusters)}\n",
    "mapped_labels = [cluster_dict[label] for label in y_test_pred]\n",
    "show_confu(y_test_pred,'MeanShift')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# Create a KNN classifier\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "# Train the KNN classifier on the PCA-transformed features\n",
    "knn_classifier.fit(X_train_pca, y_train)\n",
    "\n",
    "# Predictions on the test set\n",
    "y_test_pred = knn_classifier.predict(X_test_pca)\n",
    "\n",
    "# Evaluate the accuracy\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "show_confu(y_test_pred,'Knn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "# Create an SVM classifier\n",
    "svm_classifier = SVC(kernel='linear', C=1.0, random_state=42)\n",
    "\n",
    "# Train the SVM classifier on the PCA-transformed features\n",
    "svm_classifier.fit(X_train_pca, y_train)\n",
    "\n",
    "# Predictions on the test set\n",
    "y_test_pred = svm_classifier.predict(X_test_pca)\n",
    "\n",
    "# Evaluate the accuracy\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "show_confu(y_test_pred,'Svm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Desion Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "tree_classifier = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Train the decision tree model on the PCA-transformed features\n",
    "tree_classifier.fit(X_train_pca, y_train)\n",
    "\n",
    "# Predictions on the test set\n",
    "y_pred = tree_classifier.predict(X_test_pca)\n",
    "\n",
    "# Evaluate the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "show_confu(y_test_pred,'Desion-Tree')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
